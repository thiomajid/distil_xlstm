# Distil-xLSTM: Learning Attention Mechanisms through Recurrent Structures

This repo contains code for training and inference using an xLSTM distilled from a Transformer model with DPO and SFT. The code is based on [MambaInLlama](https://github.com/jxiw/MambaInLlama/tree/main) implementation.
