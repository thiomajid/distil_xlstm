# Custom training arguments
teacher_name: "Qwen/Qwen2.5-0.5B-Instruct"
quantize_teacher: true
xlstm_config_path: "./xlstm_config.yaml"
dataset_url: "roneneldan/TinyStories"
train_samples: 5000
eval_samples: 5000

# CE-loss args
ce_weight: 0.2
final_ce_weight: 1
ce_schedule: "no-op"

# KL-loss args
kl_weight: 0.8
final_kl_weight: 1
kl_schedule: "increase"

# Temperature args
temperature: 0.7
final_temperature: 1
temperature_schedule: "decrease"

# HF TrainerArguments arguments
num_train_epochs: 5
gradient_accumulation_steps: 1
learning_rate: 0.0001
# weight_decay: 0.001
lr_scheduler_type: "cosine"
torch_compile: false
per_device_train_batch_size: 4

output_dir: "distil-xlstm-artifacts"
logging_dir: "distil-xlstm-logs"
report_to: "tensorboard"
logging_steps: 100
save_steps: 1000
hub_private_repo: false
save_total_limit: 4
load_best_model_at_end: true
eval_strategy: "steps"

fp16: false
use_cpu: false
push_to_hub: true
resume_from_checkpoint: "distil-xlstm-artifacts"
hub_model_id: "thiomajid/distil-xlstm"
gradient_checkpointing: false
remove_unused_columns: false
