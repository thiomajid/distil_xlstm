vocab_size: 32_000
context_length: 512
num_blocks: 16
embedding_dim: 256
slstm_at: [0, 2, 4, 6, 8, 10, 12, 14]

mlstm_block:
  mlstm:
    conv1d_kernel_size: 4
    qkv_proj_blocksize: 64
    num_heads: 32

slstm_block:
  slstm:
    backend: "vanilla"
    num_heads: 32
    conv1d_kernel_size: 4
    bias_init: "powerlaw_blockdependent"

  feedforward:
    proj_factor: 1.7
    act_fn: "gelu"
