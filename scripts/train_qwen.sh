#! /bin/bash

python3 train_hf.py model=qwen1.5 \
		+hub_model_id="Qwen/Qwen1.5-MoE-A2.7B" \
		+max_seq_length=256 \
		+attn_implementation="sdpa" \
		++model.num_hidden_layers=8 \
		++model.num_experts=8 \
		++model.num_experts_per_tok=2 \
		++model.hidden_size=960 \
		++model.vocab_size=49152 \
		++model.torch_dtype="float32" \
		++trainer.hub_token="${HUB_TOKEN}" \
		++trainer.hub_model_id="${HUB_MODEL_ID}" \
		++trainer.seed=42 \
		++trainer.optim="adamw_torch_fused" \
		++trainer.lr_scheduler_type="cosine" \
		++trainer.learning_rate=5e-5 \
		++trainer.weight_decay=0.01 \
		++trainer.warmup_ratio=0.05 \
		++trainer.fp16=true \
		++trainer.num_train_epochs=1 \
		++trainer.gradient_accumulation_steps=5 \
		++trainer.per_device_train_batch_size=5 \
		++trainer.per_device_eval_batch_size=5 \
		++trainer.logging_steps=200 \
		++trainer.save_steps=200 \
		++trainer.train_dataset_url="HuggingFaceFW/fineweb-edu-llama3-annotations" \
		++trainer.train_split="train" \
		++trainer.train_samples=20000 \
		++trainer.eval_dataset_url="HuggingFaceFW/fineweb-edu" \
		++trainer.eval_split="train" \
		++trainer.eval_subset="default" \
		++trainer.eval_samples=2000 \
		++trainer.features=["text"] \
		++trainer.use_dataset_cache=true \
		++trainer.dataset_cache_dir="./.hf_data_cache"
