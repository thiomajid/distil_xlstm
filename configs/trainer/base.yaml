# Custom training arguments
teacher_name: "HuggingFaceTB/SmolLM2-135M-Instruct"
quantize_teacher: false

train_dataset_url: "roneneldan/TinyStories"
# train_subset: "default"
train_split: "train"
train_samples: 10_000

eval_dataset_url: "roneneldan/TinyStories"
# eval_subset: "CC-MAIN-2018-43"
eval_split: "validation"
eval_samples: 1000
features: ["text"]

# Loss terms weights
delta: 0.015

# cross-entropy
ce_weight: 0.6
final_ce_weight: 0.4
ce_schedule: "increasing"

# KL-divergence
kl_weight: 0.3
final_kl_weight: 0.2
kl_schedule: "decreasing"
compute_kl_loss: true

# Hidden state alignment
compute_alignment_loss: true
alignment_loss: "cosine"
alignment_weight: 0.01
final_alignment_weight: 0.001
alignment_schedule: "decreasing"
frobenius_norm_reduction: "block_wise"
additive_alignment_weight: true

# Temperature args
temperature: 2
final_temperature: 0.7
temperature_schedule: "decreasing"

# HF TrainerArguments arguments
num_train_epochs: 2
gradient_accumulation_steps: 5
per_device_train_batch_size: 4
per_device_eval_batch_size: 4

learning_rate: 0.0002
# weight_decay: 0.001
warmup_ratio: 0.1
optim: "adamw_torch_fused"
lr_scheduler_type: "cosine"
torch_compile: false
fp16: true
use_cpu: false
gradient_checkpointing: false

output_dir: "artifacts"
logging_dir: "artifacts"
report_to: "tensorboard"
logging_steps: 200
save_steps: 200
hub_private_repo: true
save_total_limit: 2
load_best_model_at_end: true
eval_strategy: "steps"

push_to_hub: true
resume_from_checkpoint: "artifacts"
hub_model_id: "thiomajid/distil_ratio"
remove_unused_columns: false
