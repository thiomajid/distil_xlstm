architectures:
- LlamaForCausalLM
attention_bias: false
attention_dropout: 0.0
bos_token_id: 1
eos_token_id: 2
head_dim: 64
hidden_act: silu
hidden_size: 960
initializer_range: 0.02
intermediate_size: 2560
is_llama_config: true
max_position_embeddings: 8192
mlp_bias: false
model_type: llama
num_attention_heads: 15
num_hidden_layers: 32
num_key_value_heads: 5
pad_token_id: 2
pretraining_tp: 1
rms_norm_eps: 1.0e-05
rope_interleaved: false
rope_scaling: null
rope_theta: 100000
tie_word_embeddings: true
torch_dtype: bfloat16
transformers.js_config:
  kv_cache_dtype:
    fp16: float16
    q4f16: float16
transformers_version: 4.47.1
use_cache: true
vocab_size: 49152
